# -*- coding: utf-8 -*-
"""INT344.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wq6VcuB2CL3sBa7r9tbwEGS0HMZKoqAk
"""

!pip install gensim tensorflow

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import string
from gensim.models import KeyedVectors
from gensim.models import Word2Vec

from google.colab import drive
drive.mount('/content/drive')

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

fake_df=pd.read_csv('/content/drive/MyDrive/fake news detection/Fake.csv')
true_df=pd.read_csv('/content/drive/MyDrive/fake news detection/True.csv')

fake_df.head()

true_df.head()

fake_df['labels']=0
true_df['labels']=1

fake_df.info()

news_df=pd.concat([fake_df, true_df],ignore_index=True)
news_df

news_df=news_df.sample(frac=1).reset_index(drop=True)

import gensim.downloader as api
wv=api.load('word2vec-google-news-300')

def text_preprocessing(text):
  #removing punctuation
  make_trans=text.maketrans('','',string.punctuation)
  text=text.translate(make_trans)
  #Word tokenize
  tokens=word_tokenize(text.lower())
  stopword=set(stopwords.words('english'))
  #removing stopwords
  filter_tokens=[token for token in tokens if token not in stopword]
  lemmatizer=WordNetLemmatizer()
  lemma_lst=[lemmatizer.lemmatize(token) for token in filter_tokens]
  if lemma_lst:
    return wv.get_mean_vector(lemma_lst)
  else:
    return np.zeros(300)

news_df.drop(columns=['title','date'],inplace=True)

news_df.head()

news_df['Vectors']=news_df['text'].apply(lambda text: text_preprocessing(text))

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(news_df['Vectors'], news_df['labels'], test_size=0.2, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

!pip install tensorflow

!pip install sequential

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout

model_cnn=Sequential()
model_cnn.add(Embedding(input_dim=5000, output_dim=128, input_length=500))
model_cnn.add(Conv1D(filters=128, kernel_size=5, activation='relu'))

# Add a max-pooling layer
model_cnn.add(MaxPooling1D(pool_size=2))

# Flatten the output from the previous layer
model_cnn.add(Flatten())
model_cnn.add(Dense(64, activation='relu'))

# Add a dropout layer to prevent overfitting
model_cnn.add(Dropout(0.5))

# Output layer (binary classification, so sigmoid activation)
model_cnn.add(Dense(1, activation='sigmoid'))
model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

X_test=np.array(X_test.tolist())
X_train=np.array(X_train.tolist())

model_cnn.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Convert lists to numpy arrays for SVM processing
X_train = np.array(X_train.tolist())
X_test = np.array(X_test.tolist())

# SVM Model
svm_model = SVC(kernel='linear')  # We can experiment with different kernels like 'rbf', 'poly', etc.
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy*100:.2f}%")

# Display the classification report
print(classification_report(y_test, y_pred))

!pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt
all_text = " ".join(news_df['text'].values)
wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=set(stopwords.words('english'))).generate(all_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
